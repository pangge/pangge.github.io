<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[goldencui]]></title>
  <subtitle><![CDATA[get a real life]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://goldencui.org/"/>
  <updated>2015-03-02T15:48:13.668Z</updated>
  <id>http://goldencui.org/</id>
  
  <author>
    <name><![CDATA[goldencwcui]]></name>
    <email><![CDATA[goldencwcui@hotmail.com]]></email>
  </author>
  
  <generator uri="http://zespia.tw/hexo/">Hexo</generator>
  
  <entry>
    <title><![CDATA[简明深度学习方法概述（三）]]></title>
    <link href="http://goldencui.org/2015/03/02/%E7%AE%80%E6%98%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <id>http://goldencui.org/2015/03/02/简明深度学习方法概述（三）/</id>
    <published>2015-03-02T15:22:15.000Z</published>
    <updated>2015-03-02T15:47:47.000Z</updated>
    <content type="html"><![CDATA[<p> <img src="/imgs/30.png" alt=""></p>
<h1 id="深度自编码———-非监督学习"><strong>深度自编码———-非监督学习</strong></h1>
<p>一下介绍了非监督学习深度网络模型，我们以此开始三种类别网络的实例化介绍。<br>深度自编码是一类特别的DNN（不含有类标签），它的输出向量和输入向量维度相同。它常常用来学习一种原始数据的表示或者有效的编码方式，并在隐藏的输入层作为向量的形式表示。注意自编码模型是一种非线性特征提取模型，没有类标签。<a id="more"></a>同样的，特征提取模型目标是保存和更好的表示信息而不是任务分类，经过某些方面来说这两个目标相互联系。<br>一个典型的自编码模型拥有一个可以表示原始数据的输入层或者说叫输入特征向量（比如，图像像素或者声音频谱），一个或者更多表示特征转换的隐藏层，以及一个匹配输入层用来重建的输出层。当隐藏层数量超过一个的时候，自编码模型就认为是深度自编码模型。隐藏层的维度既可以比输入层维度更小（目的是压缩特征）也可以更大（目的是把特征映射到更高维空间）。<br>自编码模型经常使用随机梯度下降法训练参数。尽管这样的方法很有效，但经常在训练带有多隐层的网络的时候有一些基本困难。当一些误差反馈到初始的一些层的时候，误差变得极小，训练变得无效起来。虽然有些其它的更高级的反馈传播方法某种程度上可以去解决这些困难，但是仍然导致低效，尤其是当训练数据有限的时候。但幸运的是，这个难题可以通过把每一层网络昨晚自编码模型进行预训练来解决（引用：G. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527–1554, 2006.）。这种策略已经被应用在很多方面比如：使用深度自编码去把图像映射到一个基于短二进制编码的图像检索中，编码文档（称为语意哈希）以及编码频谱类语音特征中。<br><strong>1)    使用深度自编码提取语音特征</strong><br>这里我们回顾一组工作，其发表在（L. Deng, M. Seltzer, D. Yu, A. Acero, A. Mohamed, and G. Hinton.Binary coding of speech spectrograms using a deep autoencoder. In<br>Proceedings of Interspeech. 2010.），这篇文章发展出了使用非监督方式从原始频谱语音数据中提取二进制语音编码的方法自编码模型。这种从模型中提取的依据二进制编码的离散表示法能应用在信息检索或者在语音识别中作为瓶颈特性。<br>包含256种频段和1，3,9或13帧的频谱块生成模型如下图所示。这种称为无向图模型称为Gaussian-Bernoulli RBM（高斯-伯努利受限玻尔兹曼机），它含有一个加高斯噪声的线性参数的可见层和一个包含500到3000二进制隐变量的隐层。学习Gaussian-Bernoulli RBM后，它的隐单元的响应可以作为训练其他Gaussian-Bernoulli RBM的输入。这两个Gaussian-Bernoulli RBM可以组成一个深信网络（deep belief net DBN），这篇论文中使用的DBN表示在图的左侧，两个RBMS被分开表示。<br>这个包含三层隐层的深度自编码模型使用DBN的权矩阵铺开组成。最底下一层使用权矩阵编码输入，上一层使用权矩阵倒序的编码输入。然后此深度自编码模型使用误差反馈来微调以最小化重建误差，如图右边所示。当学习过程结束的时候，一个可变长度的频谱图可以被编码和重构。<br> <img src="/imgs/31.png" alt=""><br><strong>2)    堆叠噪声自编码模型</strong><br>在早期的自编码研究中，编码层维度比输入层维度小。然而，在某些应用领域需要编码层维度更大一些，这些情况下，一些需要一些技术去防止神经网络学习一些琐碎无用的映射函数。使用更高维度隐层和编码层的一个原因是使得自编码模型捕捉到输入数据的更丰富的信息。<br>避免上面提到琐碎映射函数问题的方法包括使用稀疏约束，或者使用随机丢弃方法比如随机的强制某些数值归零，由此引入输入层或者隐层数据噪声。例如，在堆叠噪声自编码模型（详细见，P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol.<br>Stacked denoising autoencoders: Learning useful representations in a<br>deep network with a local denoising criterion. Journal of Machine<br>Learning Research, 11:3371–3408, 2010.）中，随机噪声加在输入数据里。这有几个目的，第一，强制输出匹配原始无失真输入数据，模型能避免学习到琐碎无用函数。第二，噪声随机加入后，学习到的模型在同样畸变的测试数据中将更具鲁棒性。第三，每处畸变输入样本不同，极大的提高了测试集尺寸并因此减轻过拟合线问题。<br><strong>3)    变换自编码模型</strong><br>上述深度自编码模型可以提取稳固的特征向量编码归功于模型中多层的非线性处理过程。然而，这种方法提取到的编码是多种多样的。换句话说，当测试者选择的输入特征向量变化的时候，提取的编码将会改变。有时候需要编码变化是可预测的并且反应获取内容的相对不变性。怀着这种目标的变换自编码模型在图像识别中被提出（G. Hinton, A. Krizhevsky, and S. Wang. Transforming autoencoders. In<br>Proceedings of International Conference on Artificial Neural Networks.<br>2011.）。<br>变换自编码模型的构件是一个被称为“胶囊”的东西，它是一个独立子网络，其提取一个单一参数化特征并且表示一个单一实体（一个可视的或者音频的东西）。变换自编码同时接收输入向量和目标输出向量（通过一个全局变换机制从输入向量变换得到）。一个显示表示全局变换机制假定已知。变换自编码模型的编码层由一些“胶囊”组成。<br>在训练阶段，不同的胶囊学习提取不同的实体以便最小化最终输出和目标输出的误差。<br>除了这里介绍的深度自编码结构以外，这里有很多学术上描述的生成模型只使用数据本身（无分类标签数据），用来自动获取高层特征信息。</p>
<h1 id="预训练深度神经网络———-混合体"><strong>预训练深度神经网络———-混合体</strong></h1>
<p>在此篇里，我们介绍一种广泛使用的混合深度结构————预训练深度神经网络（PDNN）并且讨论一下相关技术以及构建RBM（受限玻尔兹曼机）和DBN（上面描述过）。我们在深度神经网络（DNN）之前讨论混合的DNN实例部分是因为从非监督模型到DNN中间的混合模型再到DNN的过度比较自然。监督学习的属性已经广为人知，因此很容易理解使用了非监督预训练的DNN混合模型。<br>本节参考了最近发布的文章：<br><strong>【1】</strong>G. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent, pre-trained<br>deep neural networks for large vocabulary speech recognition. IEEE<br>Transactions on Audio, Speech, &amp; Language Processing, 20(1):30–42,<br>January 2012.<br><strong>【2】</strong>G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior,<br>V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. Deep neural<br>networks for acoustic modeling in speech recognition. IEEE Signal<br>Processing Magazine, 29(6):82–97, November 2012.<br><strong>【3】</strong>D. Yu and L. Deng. Deep learning and its applications to signal and<br>information processing. IEEE Signal Processing Magazine, pages 145–<br>154, January 2011.<br><strong>1)    限制玻尔兹曼机（RBM）</strong><br>限制玻尔兹曼机是一种特殊类型的马尔科夫随机场，它含有一层随机隐藏单元和一层随机可见单元。RBM能表示一个二分图，所有可见的单元链接到所有隐藏单元，而且没有可见—-可见或者隐藏—-隐藏之间的链接。<br><strong>2)    非监督层级预训练</strong><br>这里我们描述如何堆叠RBMs并构成DBN作为DNN预训练的基础。在讨论细节之前，我们首先注意一下由Hinton和Salakhutdinov提出的程序（【1】G. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep<br>belief nets. Neural Computation, 18:1527–1554, 2006.），这是一个更一般的非监督层级预训练。也就是说，不仅RBMs可以堆叠生成深度生成网络，其他类型的神经网络也可以做相同的事情，比如由Bengio提出了使用自编码的一个变体记性预训练生成深度网络（Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layerwise<br>training of deep networks. In Proceedings of Neural Information<br>Processing Systems (NIPS). 2006.）。<br>层次堆叠的RBM可以生成DBN，如下图就是一个这样的例子。堆叠过程如下：当学习一个高斯伯努利RBM（比如连续语音特性的应用）后，我们把当前层的响应作为下一层的训练数据。第二层的响应用作为第三层的输入数据，以此类推。这种层次堆叠的贪婪学习策略已经在文章<strong>【1】</strong>有了理论分析验证。注意这种学习过程是非监督的而且不需要类标签。<br>当用在分类任务的时候，这种预训练模型可以相互连接，并且有识别能力的学习过程可以微调所有连接权提高网络的能力。这种可识别的能力的微调通过添加表示一组表示目标输出数据的最终参数层来实现。然后，反馈调节算法可以调整网络权值来修正参数。最上一层的标签层是什么是根据DNN的应用领域的不同而定。<br><img src="/imgs/32.png" alt=""><br>基于RBM堆叠为RBM的预训练已经在大多数场合得到很好表现。需要指出的是除了这种方式以外还有很多其他方式作为预训练方法。</p>
<p><strong>3)    连接DNNS和HMMS</strong><br>目前讨论的预训练的DNN是一个显著的混合深度网络，它是一个输入向量固定维度的静态分类器。然而，很多实际的模式识别和信息处理难题，包括语音识别，机器翻译，自然语义理解，视频处理和生物信息处理需要连续识别。在连续识别中，输入层和输出层的维度是变化的。<br>基于动态编程操作的隐形马尔科夫模型（HMM）是一个解决这个问题的方便方法。因此，很自然而然的想到结合前馈神经网络和HMM桥接。一个使用DNN解决这个问题的流行结构如下图所示。这个结构成功的应用在了语音识别实验中（G. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent DBNHMMs<br>in large vocabulary continuous speech recognition. In Proceedings<br>of International Conference on Acoustics Speech and Signal Processing<br>(ICASSP). 2011.）。<br>  <img src="/imgs/33.png" alt=""></p>
<h1 id="深度堆栈网络———-监督学习"><strong>深度堆栈网络———-监督学习</strong></h1>
<p>在识别和分类任务包括语音识别和图像分类中，DNN已经展现了巨大的威力，但是训练一个DNN网络却因为复杂度高而难以计算。尤其是常见的训练DNN的技术涉及计算复杂度高的随机梯度下降法，这很难通过机器并行（CPU）来提高速度。这使得学习变成一个大规模复杂问题。现在可以使用一个单一功能强悍的GPU去训练DNN为基础的包含数十到数百数千小时训练数据的表现很好的语音识别器。然而，现在还不清楚如何使用更大规模的训练数据来提高识别的成功率。J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao,<br>M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng. Large scale<br>distributed deep networks. In Proceedings of Neural Information Processing<br>Systems (NIPS). 2012.介绍了一些最近的探索工作。<br>这里我们描述一种新的深度学习结构——-深度堆叠网络（DSN）。这小节基于最近发表的几篇文章并作进一步讨论：<br><strong>【1】</strong>L. Deng and D. Yu. Deep convex network: A scalable architecture for<br>speech pattern classification. In Proceedings of Interspeech. 2011.<br><strong>【2】</strong>L. Deng, D. Yu, and J. Platt. Scalable stacking and learning for building<br>deep architectures. In Proceedings of International Conference on<br>Acoustics Speech and Signal Processing (ICASSP). 2012a.<br><strong>【3】</strong>B. Hutchinson, L. Deng, and D. Yu. A deep architecture with bilinear<br>modeling of hidden representations: Applications to phonetic recognition.<br>In Proceedings of International Conference on Acoustics Speech<br>and Signal Processing (ICASSP). 2012.<br><strong>【4】</strong>B. Hutchinson, L. Deng, and D. Yu. Tensor deep stacking networks.<br>IEEE Transactions on Pattern Analysis and Machine Intelligence,<br>35:1944–1957, 2013.<br>DSN的主要设计概念基于堆叠方法，在L. Breiman. Stacked regression. Machine Learning, 24:49–64, 1996.中有基本概念的描述。堆叠方法首先构建一个单模型函数或分类器，然后把这些函数相互堆叠“堆叠”以便学习复杂的函数或分类器。很多实现堆叠的操作方法在最近被提了出来，主要思想是利用在单一模型中使用可监督的信息。堆叠分类器上层的分类器使用串联结构中的下层分类器的输出以及初始输入中获取新的特征。在这片论文中<br>（W. Cohen and R. V. de Carvalho. Stacked sequential learning. In<br>Proceedings of International Joint Conference on Artificial Intelligence<br>(IJCAI), pages 671–676. 2005.）堆叠模型使用的简单模型是条件随机场（CRF）。添加隐藏状态后，这类深度结构在自然语言处理和语音识别领域（这些应用的训练数据的分割信息位置）应用取得了进一步发展并取得成果。在这片论文中<br>（K. Jarrett, K. Kavukcuoglu, and Y. LeCun. What is the best multistage<br>architecture for object recognition? In Proceedings of International<br>Conference on Computer Vision, pages 2146–2153. 2009.），卷积神经网络也被当做堆叠结构，但是监督信息通常在最终的堆叠模型中并未使用。<br>DSN结构最早在论文<strong>【1】</strong>中提出，当时被作为深度凸网络，用来强调主要学习网络算法中的凸特性。DSN把监督信息用在相互堆叠的基础模型中，模型利用了多层感知器的方便性。在基础模型中，输出单元是线性的而且隐层sigmoidal（反曲函数）非线性。输出层的线性特征能使用较好的凸优化求解。由于在输入和输出之间的封闭限制，输入层权值能高效估计出来。<br>一种基础深度堆叠网络结构<br>如下图所示是一种DSN结构模型，包含可变数量的层次化模型，其中每一个模型都是由一个隐层和两组可训练的权值组成典型的神经网络结构。在图中，只展示了四个此类模型，每个模型都由不同的颜色表示。事实上，在图像和语音分类实验中有数百数千个模型被高效训练着。<br> <img src="/imgs/34.png" alt="此深度堆叠模型使用输入输出堆叠，展示了四个模型堆叠，虚线表示层的复制" title="此深度堆叠模型使用输入输出堆叠，展示了四个模型堆叠，虚线表示层的复制"><br>DSN结构中最低一层模型包含一个线性输入单元的输入层，一个非线性单元的非线性隐层和第二个线性输出单元的线性层。隐藏层使用了sigmoidal非线性函数（当然可以使用其他非线性函数，原理相同，非线性函数保证模型训练出的映射的非线性，否则训练出的映射是线性）。如果DSN用在识别一个图像，输入单元可以适应一组图像像素（或者一组图像特征）。如果用在语音识别，输入单元可以适应一组语音波形样本或者提取的语音波形特征，比如功率谱和倒谱系数。输出单元表示目标的分类信息。例如， DSN用来识别数字，输出可能表示0,1,2,3 等等的数值并采用二进制编码的形式。<br>较低层的权值矩阵我们可以用W表示，连接了线性输入单元和非线性隐层单元。上一层权值矩阵用U表示，连接非线性隐层和线性输出层。权值矩阵U可根据W的值采用均方差训练方法确定封闭解。<br>如上所述，DSN包括一组连接，重叠和层次的模型。其中每个模型有相同的结构。注意的是底层输出单元是临近高层模型的输入单元的子集。具体来说，DSN靠上的一层的输入层包含底层输出层的单元，逻辑上说也包括初始时候的原始图像特征。<br>一个学习好的DSN可以部署在自动分类任务例如帧水平语音通话或者状态分类中。把DSN的输出层连接到HMM或者任何动态可编程设备可以进行连续语音识别任务和其他类型序列模式识别任务。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p> <img src="/imgs/30.png" alt=""></p>
<h1 id="深度自编码———-非监督学习"><strong>深度自编码———-非监督学习</strong></h1>
<p>一下介绍了非监督学习深度网络模型，我们以此开始三种类别网络的实例化介绍。<br>深度自编码是一类特别的DNN（不含有类标签），它的输出向量和输入向量维度相同。它常常用来学习一种原始数据的表示或者有效的编码方式，并在隐藏的输入层作为向量的形式表示。注意自编码模型是一种非线性特征提取模型，没有类标签。]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://goldencui.org/tags/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://goldencui.org/tags/Deep-Learning/"/>
    
      <category term="DL&amp;ML" scheme="http://goldencui.org/categories/DL&ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[flask css不能更新的问题]]></title>
    <link href="http://goldencui.org/2014/12/07/flask-css%E4%B8%8D%E8%83%BD%E6%9B%B4%E6%96%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://goldencui.org/2014/12/07/flask-css不能更新的问题/</id>
    <published>2014-12-07T13:18:54.000Z</published>
    <updated>2014-12-07T13:40:34.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/imgs/flasklabel.jpg" alt="flask"><br>最近在不误正业的做一个周末项目。尝试用flask搭建一个小项目网站，我在windows下使用flask，其中遇到一个问题，当第一次写好css文件之后，网页特效现实正常。然后，当我更新css文件的细节之后，我还是只能看到第一次css的效果。我试过各种方法，比如查看进程重启flask，重启浏览器，甚至移植到linux下。。。（可惜还是同样的问题，困扰了很久。。。）最后我想到了会不会是浏览器缓存问题。结果不出所料，搜了stackoverflow发现别人也遇到了一样的问题。。。很好，国内网站果然比较水，完全查不到相关的细节。看看下面<a href="http://stackoverflow.com/questions/21714653/flask-css-not-updating" target="_blank" rel="external">链接</a>：<br><a id="more"></a><br> <img src="/imgs/flaskcss.jpg" alt="截图"><br>看来是flask固有问题，在flask官方文档里给出了<a href="http://flask.pocoo.org/snippets/40/" target="_blank" rel="external">解决办法</a>：<br><img src="/imgs/flaskcss1.jpg" alt="截图"><br>嗯，就是这些，大家自己感受吧。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/imgs/flasklabel.jpg" alt="flask"><br>最近在不误正业的做一个周末项目。尝试用flask搭建一个小项目网站，我在windows下使用flask，其中遇到一个问题，当第一次写好css文件之后，网页特效现实正常。然后，当我更新css文件的细节之后，我还是只能看到第一次css的效果。我试过各种方法，比如查看进程重启flask，重启浏览器，甚至移植到linux下。。。（可惜还是同样的问题，困扰了很久。。。）最后我想到了会不会是浏览器缓存问题。结果不出所料，搜了stackoverflow发现别人也遇到了一样的问题。。。很好，国内网站果然比较水，完全查不到相关的细节。看看下面<a href="http://stackoverflow.com/questions/21714653/flask-css-not-updating" target="_blank" rel="external">链接</a>：<br>]]>
    
    </summary>
    
      <category term="flask" scheme="http://goldencui.org/tags/flask/"/>
    
      <category term="css" scheme="http://goldencui.org/tags/css/"/>
    
      <category term="python" scheme="http://goldencui.org/tags/python/"/>
    
      <category term="python" scheme="http://goldencui.org/categories/python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[简明深度学习方法概述（二）]]></title>
    <link href="http://goldencui.org/2014/12/06/%E7%AE%80%E6%98%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://goldencui.org/2014/12/06/简明深度学习方法概述（二）/</id>
    <published>2014-12-06T02:00:08.000Z</published>
    <updated>2014-12-06T02:24:06.000Z</updated>
    <content type="html"><![CDATA[<p> <img src="/imgs/nn2.jpg" alt=""></p>
<h1 id="三类深度学习网络"><strong>三类深度学习网络</strong></h1>
<p>本篇介绍深度学习的大致分类，根据其应用方式的不同，我们可以粗分类几种不同结构的深度学习方法。</p>
<h2 id="(1)_三种深度学习网络分类方式">(1)   三种深度学习网络分类方式</h2>
<p><a id="more"></a><br>如文章（一）中所述，深度网络是指一大类的机器学习和各种层次结构结合的网络，其特性是使用多层的非线性信息处理方法（这和一般神经网络结构类似，包含了更多的隐层）。根据这些结构和技术的应用领域比如综合/生成或识别/分类，我们可以大致的把这些结构分为三类：</p>
<ol>
<li><strong>非监督学习或生成学习深度网络</strong>，当目标的类标签信息不可获取的时候，这类深度网络趋向于为了模式分析和综合的目的，提取可视数据中高度自相关性（可参考机器学习中对于非监督学习的描述）。在学术上，非监督特征或者表示学习指的就是这类深度网络。当用在生成模型中的时候，很多这类模型也用来描述可视数据的统计分布和他们的相关类属的分布，并把它们作为数据的一部分。</li>
<li><strong>监督学习深度网络</strong>，这类模型用在直接提供模式分类中的识别能力，经常以描述可视数据的类的后验分布形式给出。类标签可以直接或间接得到（因为是监督学习）。</li>
<li><strong>混合深度网络</strong>，目标是辅加生成学习或者深度非监督学习效果的数据分类问题。一般可以使用最优化和第2类深度网络模型来解决。这种网络使用监督学习的标准去估计任何深度生成模型或者非监督深度网络中的参数。</li>
</ol>
<p>“混合”这一称呼和一般学术指的不同，学术上可能指把一个神经网络的概率输出反馈到HMM（隐形马尔科夫模型，推荐李航的《统计学习方法》中了解）中所组成的语音识别系统。<br>如果按照普通的采用机器学习的惯例，那就可以很自然的把深度学习技术分为深度判别模型（监督学习）（比如深度神经网络或者DNNS，递归神经网络或者RNNs，卷积神经网络或者CNNs等等）和生成或非监督模型（比如限制玻尔兹曼机或者RBMs，深信网络或者DBNs，深度玻尔兹曼机（DBMs）规则自编码器等等）。但这两种分类方式都忽略了生成或非监督模型通过最优化和正则化可以极大的改进DNNs和其他深度判定模型和监督模型的训练。并且，深度无监督学习网络未必能从数据中有意义的采样。我们注意到已经有研究通过生成传统的去噪自编码器来来有效采样（具体论文不列出，大家可以去搜搜看）。不管怎么说，传统的两种分类方式的确指出了监督学习和非监督学习深度网络中的不同点。比较上面提到的两类深度学习方法，深度监督学习方法例如DNNs是通常可以有效测试和训练的模型，可以灵活构建并且适合复杂系统的首位相连的学习（比如loopy belief propagation）。另一方面来说，深度非监督学习模型，特别是概率生成模型，更容易解释，容易嵌入局部先验知识，容易组织和容易处理不确定性。但是它在推理学习和复杂系统学习方面比较困难。这些区分也保留在本文推荐的三类深度网络模型分类里面，这是一个好现象。</p>
<h2 id="(2)_非监督学习或生成学习深度网络">(2)      非监督学习或生成学习深度网络</h2>
<p>非监督学习指学习没有监督信息的数据（比如根本没有数据的类标签，无法根据数据的类属性分类）。很多这类深度网络可以从神经网络知识里借鉴生成，比如RBMs，DBNs，DBMs和生成去噪自编码器。还有一些比如稀疏编码网络以及深度自编码器的原始形式。下面给出这类网络的样例以及相关文献信息，具体内容可以根据兴趣学习：</p>
<p><em>[1]    Y. Bengio, N. Boulanger, and R. Pascanu. Advances in optimizing recurrent networks. In Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP). 2013<br>[2]    Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layerwise training of deep networks. In Proceedings of Neural Information Processing Systems (NIPS). 2006.<br>[3]    Y. Bengio. Learning deep architectures for AI. in Foundations and Trends in Machine Learning, 2(1):1–127, 2009.<br>[4]    Y. LeCun, S. Chopra, M. Ranzato, and F. Huang. Energy-based models in document recognition and computer vision. In Proceedings of International Conference on Document Analysis and Recognition (ICDAR).2007.<br>[5]    J. Ngiam, Z. Chen, P. Koh, and A. Ng. Learning deep energy models. In Proceedings of International Conference on Machine Learning (ICML).2011.<br>[6]    L. Deng, M. Seltzer, D. Yu, A. Acero, A. Mohamed, and G. Hinton.Binary coding of speech spectrograms using a deep autoencoder. In Proceedings of Interspeech. 2010.<br>[7]    G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, July 2006.<br>[8]    P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. Manzagol.Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11:3371–3408, 2010.<br>[9]    G. Hinton. A better way to learn features. Communications of the Association for Computing Machinery (ACM), 54(10), October 2011.<br>[10]    I. Goodfellow, M. Mirza, A. Courville, and Y. Bengio. Multi-prediction deep boltzmann machines. In Proceedings of Neural Information Processing Systems (NIPS). 2013.<br>[11]    R. Salakhutdinov and G. Hinton. Deep boltzmann machines. In Proceedings of Artificial Intelligence and Statistics (AISTATS). 2009.<br>[12]    R. Salakhutdinov and G. Hinton. A better way to pretrain deep Boltzmann machines. In Proceedings of Neural Information Processing Systems (NIPS). 2012.<br>[13]    N. Srivastava and R. Salakhutdinov. Multimodal learning with deep boltzmann machines. In Proceedings of Neural Information Processing Systems (NIPS). 2012.<br>[14]    G. Dahl, M. Ranzato, A. Mohamed, and G. Hinton. Phone recognition with the mean-covariance restricted boltzmann machine. In Proceedings of Neural Information Processing Systems (NIPS), volume 23, pages 469–477. 2010.<br>[15]    M. Ranzato and G. Hinton. Modeling pixel means and covariances using factorized third-order boltzmann machines. In Proceedings of Computer Vision and Pattern Recognition (CVPR). 2010.<br>[16]    A. Mohamed, G. Hinton, and G. Penn. Understanding how deep belief networks perform acoustic modelling. In Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP). 2012.<br>[17]    R. Gens and P. Domingo. Discriminative learning of sum-product networks.Neural Information Processing Systems (NIPS), 2012.<br>[18]    H. Poon and P. Domingos. Sum-product networks: A new deep architecture.In Proceedings of Uncertainty in Artificial Intelligence. 2011.<br>[19]    J. Martens and I. Sutskever. Learning recurrent neural networks with hessian-free optimization. In Proceedings of International Conference on Machine Learning (ICML). 2011.<br>[20]    Y. Bengio. Deep learning of representations: Looking forward. In Statistical Language and Speech Processing, pages 1–37. Springer, 2013.<br>[21]    I. Sutskever. Training recurrent neural networks. Ph.D. Thesis, University of Toronto, 2013.<br>[22]    T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khudanpur. Recurrent neural network based language model. In Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP), pages 1045–1048. 2010.<br>[23]    G. Mesnil, X. He, L. Deng, and Y. Bengio. Investigation of recurrentneural-network architectures and learning methods for spoken language understanding. In Proceedings of Interspeech. 2013.<br>[24]    K. Yao, G. Zweig, M. Hwang, Y. Shi, and D. Yu. Recurrent neural networks for language understanding. In Proceedings of Interspeech.2013.<br>[25]    Z. Ling, L. Deng, and D. Yu. Modeling spectral envelopes using restricted boltzmann machines for statistical parametric speech synthesis.In International Conference on Acoustics Speech and Signal Processing (ICASSP), pages 7825–7829. 2013.<br>[26]    M. Shannon, H. Zen, and W. Byrne. Autoregressive models for statistical parametric speech synthesis. IEEE Transactions on Audio, Speech,Language Processing, 21(3):587–597, 2013.<br>[27]    H. Zen, Y. Nankaku, and K. Tokuda. Continuous stochastic feature mapping based on trajectory HMMs. IEEE Transactions on Audio,Speech, and Language Processings, 19(2):417–430, February 2011.<br>[28]    M. Wohlmayr, M. Stark, and F. Pernkopf. A probabilistic interaction model for multi-pitch tracking with factorial hidden markov model.IEEE Transactions on Audio, Speech, and Language Processing, 19(4),May 2011.<br>[29]    R. Socher, C. Lin, A. Ng, and C. Manning. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of International Conference on Machine Learning (ICML). 2011.<br>[30]    R. Socher, Y. Bengio, and C. Manning. Deep learning for NLP.Tutorial at Association of Computational Logistics (ACL), 2012, and North American Chapter of the Association of Computational Linguistics (NAACL), 2013.<br><a href="http://www.socher.org/index.php/DeepLearning" target="_blank" rel="external">http://www.socher.org/index.php/DeepLearning</a> Tutorial.</em><br>（有点多。。。大家可以根据需要检索，不过都是比较新的索引文献，值得一读）</p>
<h2 id="(3)_监督学习深度网络">(3)      监督学习深度网络</h2>
<p>很多信号和信息处理方面的监督学习的判别技术都是浅层结构比如HMMs（隐形马尔科夫模型）和条件随机场（CRFs），条件随机场本质上就是一个浅层判别模型，在输入特征和多度特征上用线性关系描述。最近深度组织CRFs被提出来，它存储每一层CRF的输出，和原始输入数据一起传输到它的更高层。各种版本的深度组织CRFs已经成功的应用在了电话语音识别，语音识别和自然语言处理上。下面给出监督学习相关的深度网络方面的参考文献，具体内容大家根据兴趣查找阅读：</p>
<p><em>[1]    M. Gibson and T. Hain. Error approximation and minimum phone error acoustic model estimation. IEEE Transactions on Audio, Speech, and Language Processing, 18(6):1269–1279, August 2010.<br>[2]    Heintz, E. Fosler-Lussier, and C. Brew. Discriminative input stream combination for conditional random field phone recognition. IEEE Transactions on Audio, Speech, and Language Processing, 17(8):1533–1546, November 2009.<br>[3]    G. Heigold, H. Ney, P. Lehnen, T. Gass, and R. Schluter. Equivalence of generative and log-liner models. IEEE Transactions on Audio, Speech,and Language Processing, 19(5):1138–1148, February 2011.<br>[4]    D. Yu, S. Wang, and L. Deng. Sequential labeling using deep-structured conditional random fields. Journal of Selected Topics in Signal Processing,4:965–973, 2010.<br>[5]    D. Yu and L. Deng. Deep-structured hidden conditional random fields for phonetic recognition. In Proceedings of Interspeech. September 2010.<br>[6]    D. Yu, S. Wang, and L. Deng. Sequential labeling using deep-structured conditional random fields. Journal of Selected Topics in Signal Processing,4:965–973, 2010.<br>[7]    N. Morgan. Deep and wide: Multiple layers in automatic speech recognition.IEEE Transactions on Audio, Speech, &amp; Language Processing,20(1), January 2012.<br>[8]    L. Deng and D. Yu. Deep convex network: A scalable architecture for speech pattern classification. In Proceedings of Interspeech. 2011<br>[9]    L. Deng, D. Yu, and J. Platt. Scalable stacking and learning for building deep architectures. In Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP). 2012a.<br>[10]    B. Hutchinson, L. Deng, and D. Yu. A deep architecture with bilinear modeling of hidden representations: Applications to phonetic recognition.In Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP). 2012.<br>[11]    L. Deng, G. Tur, X. He, and D. Hakkani-Tur. Use of kernel deep convex networks and end-to-end learning for spoken language understanding.In Proceedings of IEEE Workshop on Spoken Language Technologies.December 2012.<br>[12]    L. Deng, K. Hassanein, and M. Elmasry. Analysis of correlation structure for a neural predictive model with application to speech recognition.Neural Networks, 7(2):331–339, 1994.<br>[13]    A. Graves. Sequence transduction with recurrent neural networks. Representation<br>[14]    Learning Workshop, International Conference on Machine Learning (ICML), 2012<br>[15]    A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In Proceedings of International Conference on Acoustics Speech and Signal Processing (ICASSP). 2013.</em></p>
<h2 id="(4)_混合深度网络">(4)    混合深度网络</h2>
<p>“混合”这个名词使用在这个类别里正是指一类既包含和利用了生成模型也用了判别模型的一种深度网络。在目前学术圈发表的混合结构模型中，生成组件最常被用来帮助判别模型，判别模型是这个混合模型的最终目标。生成模型如何促进判别模型可以从下列两种观点来佐证：</p>
<ol>
<li><strong>最优化观点</strong>，生成模型就是在非监督数据集上根据这种观点被训练的。它为高度非线性参数估计难题提供了极好的初始化参数（通常深度网络中使用的名词“预训练”就是因此被引入的）。</li>
<li><strong>正则化观点</strong>，据此非监督学习模型能有效的提供一个在一组模型函数集上的先验。</li>
</ol>
<p>下面提供关于这类模型相关的参考文献，相信大家可以从中学习到比较深入的知识。</p>
<p><em>[1]    D. Erhan, Y. Bengio, A. Courvelle, P.Manzagol, P. Vencent, and S. Bengio.Why does unsupervised pre-training help deep learning? Journal on Machine Learning Research, pages 201–208, 2010.<br>[2]    D. Erhan, Y. Bengio, A. Courvelle, P.Manzagol, P. Vencent, and S. Bengio.Why does unsupervised pre-training help deep learning? Journal on Machine Learning Research, pages 201–208, 2010.<br>[3]    A. Mohamed, D. Yu, and L. Deng. Investigation of full-sequence training of deep belief networks for speech recognition. In Proceedings of Interspeech. 2010.<br>[4]    B. Kingsbury, T. Sainath, and H. Soltau. Scalable minimum bayes risk training of deep neural network acoustic models using distributed hessian-free optimization. In Proceedings of Interspeech. 2012.<br>[5]    M. Ranzato, J. Susskind, V. Mnih, and G. Hinton. On deep generative models with applications to recognition. In Proceedings of Computer Vision and Pattern Recognition (CVPR). 2011.<br>[6]    H. Lee, R. Grosse, R. Ranganath, and A. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations.In Proceedings of International Conference on Machine Learning (ICML). 2009.</em><br>在接下来的一篇文章中，我们将详细说明三种深度学习领域典型类型的模型。给出简单的结构描述和数学描述。这三种举例的模型可能不一定是最具代表性和有影响力的模型，不过可以作为一种说明来让大家明白区别。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p> <img src="/imgs/nn2.jpg" alt=""></p>
<h1 id="三类深度学习网络"><strong>三类深度学习网络</strong></h1>
<p>本篇介绍深度学习的大致分类，根据其应用方式的不同，我们可以粗分类几种不同结构的深度学习方法。</p>
<h2 id="(1)_三种深度学习网络分类方式">(1)   三种深度学习网络分类方式</h2>
<p>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://goldencui.org/tags/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://goldencui.org/tags/Deep-Learning/"/>
    
      <category term="DL&amp;ML" scheme="http://goldencui.org/categories/DL&ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[简明深度学习方法概述（一）]]></title>
    <link href="http://goldencui.org/2014/12/02/%E7%AE%80%E6%98%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://goldencui.org/2014/12/02/简明深度学习方法概述（一）/</id>
    <published>2014-12-02T10:09:00.000Z</published>
    <updated>2014-12-02T11:30:52.000Z</updated>
    <content type="html"><![CDATA[<p> <img src="/imgs/nn0.jpg" alt=""><br><em>说明：本文主要是翻译整理Li Deng 和 Dong Yu所著的《Deep Learning：Methods and Application》文章并没有全文翻译，而是一个总结并加入个人理解生成的概括性文章。如果要深入了解推荐读原文。博主真心能力有限，所以理解之处错误在所难免，请勿喷。</em></p>
<h1 id="一、_综述"><strong>一、    综述</strong></h1>
<p><a id="more"></a><br>在这片介绍性文章开始，先简单介绍一下深度学习的概念。深度学习（Deap Learning），是机器学习的一个新的研究领域，它的定义有很多，这里随意列举一两个定义你们感受一下（老外的语言还是很干练的）：</p>
<p><em>1) “A class of machine learning techniques that exploit many layers of non-linear information processing for supervised or unsupervised feature extraction and transformation, and for pattern analysis and classification.”<br>2) “deep learning is a set of algorithms in machine learning that attempt to learning in multiple levels, corresponding to different levels of abstraction. It typically uses artificial neural networks. The levels in these learned statistical models correspond to distinct levels of concepts, where higher-level concepts are defined from lower-level ones, and the same lower-level concepts can help to define many higher-level concepts”</em><br>上述英文定义的共同点包括两个主要方面：</p>
<ol>
<li>模型由多个层次或多个非线性信息处理模块阶段组成</li>
<li>特征表示的监督和非监督学习方法在深度模型的更高抽象层次里</li>
</ol>
<p>深度学习是多学科领域的交叉，比如神经网络、人工智能、图建模、最优化理论、模式识别和信号处理。需要注意的是本文所描述的深度学习是在信号和信息处理内容中学习出一种深度结构。它不是对信号和信息处理知识的理解，尽管某些意义上说它俩相似，但深度学习重点在于学习出一种深度网络结构，是实实在在存在的一种计算机可存储结构，这种结构表示了信号的某种意义上的内涵。<br>从06年开始，深度结构学习方法（深度学习或者分层学习方法）作为机器学习领域的新的研究方向出现。由于三种主要领域的技术进步（比如芯片处理性能的巨大提升，数据爆炸性增长和机器学习与信信号处理研究的进步），在过去的短短几年时间，深度学习技术得到快速发展，已经深深的影响了学术领域，其研究涉及的应用领域包括计算机视觉、语音识别、对话语音识别、图像特征编码、语意表达分类、自然语言理解、手写识别、音频处理、信息检索、机器人学。<br>由于深度学习在众多领域表现比较好的性能，越来越多的学术机构把目光投入深度学习领域。今年来活跃在机器学习领域的研究机构包括众多高校比如斯坦福，伯克利，还有一些企业例如Google，IBM 研究院，微软研究院，FaceBook，百度等等。这些研究机构在计算机领域的众多应用中都成功利用了深度学习方法，甚至有一个关于分子生物学的研究指出他们利用深度学习方法引领下发现了新的药物。<br>本文只是阐述了截止2014年最新的有关深度学习研究的一部分内容综述，如果需要了解这个领域的最新进展，推荐到以下网址获取：</p>
<ul>
<li><a href="http://deeplearning.net/reading-list/" target="_blank" rel="external">http://deeplearning.net/reading-list/</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL_Recommended_Readings" target="_blank" rel="external">http://ufldl.stanford.edu/wiki/index.php/UFLDL_Recommended_Readings</a></li>
<li><a href="http://www.cs.toronto.edu/∼hinton/" target="_blank" rel="external">http://www.cs.toronto.edu/∼hinton/</a></li>
<li><a href="http://deeplearning.net/tutorial/" target="_blank" rel="external">http://deeplearning.net/tutorial/</a></li>
<li><a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial" target="_blank" rel="external">http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial</a></li>
</ul>
<h1 id="二、_深度学习历史"><strong>二、    深度学习历史</strong></h1>
<p>直到近些年，大多数机器学习和信号处理技术大多还是采用浅层的结构，这些典型结构包含至多一层或两层非线性特征变换。这种浅层结构的代表比如高斯混合模型（GMM），线性或非线性动态系统，条件随机场（CRFs），最大熵模型，支持向量机（SVMs），逻辑回归（LR），核回归，多层感知器（MLPS）。例如，SVMs 使用了一个浅层的线性模式分类器，当使用核技巧的话，包含一个特征转换层。浅层结构方法（既机器学习方法）已经在一些简单和有限制难题中得到了比较好的结果，但是当处理复杂的现实世界的问题时（例如语音，自然声音图像，语言，视场等），它们有限的模型复杂度和表达能力就遇到了困难。<br>人类自我信息的处理和理解（例如视觉信息，声音信息），一直以来都比较复杂，因此需要更深的结构算法从输入层中提取特征。比如说，语音生成和理解系统在把波形信号转变成语言级别的信号的时候，就是设置了清晰的多层结构去学习。历史上来看，深度学习的概念起源于神经网络的研究。含有多隐层的前馈神经网络（BP）或者多层感知器（多隐层MLPs通常指深度神经网络DNNs），就是一个深度结构模型的例子。BP神经网络流行在上世纪80年的，已经成为广为人知的学习算法。遗憾的是，加入多隐层的BP神经网络算法效果却并不好（网络中普遍存在的非凸目标函数的局部最优化调整问题是主要的训练难题）。随着网络深度的增加，越难达到局部最优化。而这个困境的原因是忽视了机器学习和信号处理领域的研究，比如机器学习方法中的SVM,CRF,和最大熵模型，包含损失函数，使用这些方法可以有效获得全局优化。<br>深信网络（deep belief network DBN）提出后，深度模型的最优化困难可以在经验上得到降低。DBN是由一组限制玻尔兹曼机（RBMs）组成的层次网络学习算法，它可以在线性的时间复杂度内达到模型参数的最优化。使用MLP采用合理配置初始化权值后，DBN经常能表现的比随机参数更好一点。同理，含多隐层的MLPs 或者深度神经网络（DNN）学术上也被称为DBNs。最近，研究人员已经更精细的区分DNNs和DBNs，如果使用DBN去初始化DNN的训练的话，那么这个网络就可以被称为DBN-DNN。这上述的深度学习的理论提出后，学术界不断提出改进的理论来丰富深度学习的内容，深度学习理论已经得到了极大的丰富和发展。<br>我们可以从另一个角度来了解这个发展历程，下图展现了不同时代的神经网络被宣传的热度。巅峰期出现在1980s和1990s，此时被称为神经网络的第二代。DBN在06年被研究出来，当DBN被用在初始化DNN的时候，学习算法的效率就变的更加有效，这促进了学术界连续的快速研究成果。DBN和DNN的产业级语音特征提取和识别应用出现在09年，当时产业界和学术界以及深度学习的研究专家有着密切的相互合作。这种合作快速发展了语音识别的深度学习方法，并由此而取得了巨大成功。<br> <img src="/imgs/nn1.png" alt="industry scale"><br>图中“plateau of productivity（稳定产出）”短语目前还没有到来，但是期望在未来将会比图中显示出的走势更加剧烈，像标记中的虚线那样，而我们就是刚刚处于这个时期，尤其最近深度学习概念屡屡被热炒，其发展热度可见一斑。</p>
<p><em>未完待续…</em></p>
]]></content>
    <summary type="html">
    <![CDATA[<p> <img src="/imgs/nn0.jpg" alt=""><br><em>说明：本文主要是翻译整理Li Deng 和 Dong Yu所著的《Deep Learning：Methods and Application》文章并没有全文翻译，而是一个总结并加入个人理解生成的概括性文章。如果要深入了解推荐读原文。博主真心能力有限，所以理解之处错误在所难免，请勿喷。</em></p>
<h1 id="一、_综述"><strong>一、    综述</strong></h1>
<p>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://goldencui.org/tags/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://goldencui.org/tags/Deep-Learning/"/>
    
      <category term="DL&amp;ML" scheme="http://goldencui.org/categories/DL&ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[程序员与画家]]></title>
    <link href="http://goldencui.org/2014/10/30/%E7%A8%8B%E5%BA%8F%E5%91%98%E4%B8%8E%E7%94%BB%E5%AE%B6/"/>
    <id>http://goldencui.org/2014/10/30/程序员与画家/</id>
    <published>2014-10-30T02:46:05.000Z</published>
    <updated>2014-10-30T03:32:14.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/imgs/h.jpg" alt="" title="黑客与画家"><br>程序猿与画家到底什么联系？《黑客与画家》这本书应该给了一些提示。写程序的大抵在程序从需求分析，到编码，到调试，到测试，到最终验收的各个阶段，其实都和绘画这种古老有趣的艺术是相同的。绘画也要经历构思，构图，绘制，修改，到最后成稿。这期间的种种，都包含程序员和画家的心血和想象力。<br><a id="more"></a><br>有时候我会想程序员就是一个艺术家，程序员在世间的地位往往被形容为码农，和搬砖的一个地位，表面总是不会光鲜靓丽，而我们也乐此不疲的自黑。但是和画家一样，在笔触描绘<br>的一点一顿中，画家把自己思想，把自己的情感融入一幅幅绚丽多彩的画面中，把自己的生活感悟生成永恒的画面。程序员则融入自己的心血和创造力，一步步的逻辑和点滴的巧妙设置，就如同画家对阴影，细节的处理一样，虽然没有画笔表现出的视觉直观，但是隐藏在程序背后的二进制，是世界上最精巧的艺术！它隐藏了细节，隐藏了内部的惊奇与壮观，给了你最直观的壮观的体验和方便。<br>程序员骨子里是文艺的，或者本质上，一个优秀的程序员，就和艺术家一样，他们创造，他们敢于质疑，他们表现他们手上最酷的东西，梵高在最窘迫的时候，仍然把自己的创造力发挥的淋漓尽致，优秀的程序员在几尺见方的格子里，消耗自己的健康和薯片，改变世界，一流的程序员总能改变世界的。<br>现在想来，我最早的痴迷绘画，真是一种宿命一样，我喜欢绘画，喜欢埋头填充白色纸面的那种感觉。当我拿起画笔的时候，时间就像静止一样，我可以在那种万籁俱静一样的另一个时空里，感受内心的平静，那个时候我是最本真的我。最早的梦想就是成为一个画家，像莫奈和毕加索，想达芬奇。但谁能想到我未来的职业是和程序算法相关呢？绘画给了我另一种体会世界的方式和眼光，他给了我在喧嚣世界里掩藏内心，不忘初心的方式。算法和程序给了认识世界的另一种方式，这个世界除了彩色和画面构成以外，还需要自然规律和逻辑支撑 。<br>下面是我闲事的随笔作品，我会坚持画下去，虽然距离当一名画家的梦想太过遥远。不过套用一句时髦的话，梦想还是要有的，万一实现了呢?</p>
<hr>
<h2><img src="/imgs/1.jpg" alt="余晖" title="余晖"></h2>
<h2 id="-1"><img src="/imgs/2.jpg" alt="梦境" title="梦境"></h2>
<h2 id="-1"><img src="/imgs/3.jpg" alt="鲸鱼" title="鲸鱼"></h2>
<p><img src="/imgs/4.jpg" alt="夜" title="夜"></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/imgs/h.jpg" alt="" title="黑客与画家"><br>程序猿与画家到底什么联系？《黑客与画家》这本书应该给了一些提示。写程序的大抵在程序从需求分析，到编码，到调试，到测试，到最终验收的各个阶段，其实都和绘画这种古老有趣的艺术是相同的。绘画也要经历构思，构图，绘制，修改，到最后成稿。这期间的种种，都包含程序员和画家的心血和想象力。<br>]]>
    
    </summary>
    
      <category term="艺术" scheme="http://goldencui.org/tags/%E8%89%BA%E6%9C%AF/"/>
    
      <category term="随笔" scheme="http://goldencui.org/categories/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[网络资源搜索爬虫(python 3.4.1实现)]]></title>
    <link href="http://goldencui.org/2014/10/15/%E7%BD%91%E7%BB%9C%E8%B5%84%E6%BA%90%E6%90%9C%E7%B4%A2%E7%88%AC%E8%99%AB(python%203.4.1%E5%AE%9E%E7%8E%B0)/"/>
    <id>http://goldencui.org/2014/10/15/网络资源搜索爬虫(python 3.4.1实现)/</id>
    <published>2014-10-15T02:16:50.000Z</published>
    <updated>2014-10-15T08:24:18.000Z</updated>
    <content type="html"><![CDATA[<p><img src="http://abstrusegoose.com/strips/how_stuff_works.png" alt="" title="silly man"><br>最近在学习python语言，python以前没有接触，只用过perl，以前使用perl做一些大的数据集处理，当时也是比较生疏，所以一上来看了简单的官方说明文档，就马上开始coding，大约一周基本就对perl的特性比较熟悉了。所以这次我秉持着从实践中学习技术的角度，打算用python做一些小程序，顺便熟悉python语言的各个方面的特性，也因为我对网络爬虫一直都很有些兴趣，就打算着手做个小工具。<br><a id="more"></a><br>使用python做网络爬虫，网上的资源很多，我搞不清为什么很多人和机构都热衷于用python做网络爬虫，大概是因为python在这方面提供的支持库比较多也比较容易实现吧。现有的比较典型的开源爬虫架构如<a href="http://scrapy.org/" target="_blank" rel="external">scrapy</a>（python实现），其实现的功能已经比较全面了，最早的时候想了解网络爬虫的原理的时候，曾经尝试过使用scrapy定制，scrapy已经实现了比较复杂的爬虫功能，官方文档也介绍的很详细。不过为了满足我重复造轮子的好奇心，决定自己做一下,多给脑子里填一些东西</p>
<p>python实现网络爬虫的原理和架构网上资源很多，我就不在这里赘述，大家可以参考这些个链接了解：<br>1.<a href="http://www.zhihu.com/question/20899988" target="_blank" rel="external">如何入门网络爬虫?</a><br>2.<a href="http://www.zhihu.com/question/21358581" target="_blank" rel="external">你是如何开始能写python爬虫？</a></p>
<p>用python 3做网络爬虫可以使用基本的http库也可以使用urllib（注意在python2.7.*以前都是urllib2，更新后urllib2被丢弃）这种库提供对网页url的处理模块。网上充斥着大量的爬虫教程绝大多数部分描述的是使用python2 +urllib2库。使用python3做爬虫还比较少，基本原理是一样的，不过就是urlib库里一些功能的实现和老版本的库稍有不同。<br>简单的使用urllib抓取网页的例子如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> urllib.request</div><div class="line">f = urllib.request.urlopen(<span class="string">'http://www.python.org/'</span>)</div><div class="line">print(f.read().decode(<span class="string">'urf-8'</span>))</div></pre></td></tr></table></figure>

<p>urllib的具体使用接口和方法参加<a href="https://docs.python.org/3/library/urllib.request.html#module-urllib.request" target="_blank" rel="external">官方文档</a>，这里不再详述。<br>对于网络爬虫最很重要的一环，网页页面HTML的处理方法，python官方给出了一些标准库，不过有一个第三方库<a href="http://beautifulsoup.readthedocs.org/en/latest/" target="_blank" rel="external">beautiful soup</a>对抓取的网页分析更加方便，本文所述的程序采用这个库做HTML页面分析和处理。<br>爬虫关键的就是性能问题，影响性能的原因一个是因为爬虫程序搜索网页的逻辑本身耗时，另一个是抓取页面响应时的耗时，前一种耗时可以采用python标准库中的多线程对爬虫程序进行优化，提取主页中的关键URL采用多个爬虫线程进行爬取。针对后一种耗时，可以采用集群的方式对爬虫进行优化，不过本文研究的程序仅仅作为一种学习，不深入讨论。这里指给出优化的一小部分，python多线程的实例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> threading, zipfile</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AsyncZip</span><span class="params">(threading.Thread)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, infile, outfile)</span>:</span></div><div class="line">        threading.Thread.__init__(self)</div><div class="line">        self.infile = infile</div><div class="line">        self.outfile = outfile</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></div><div class="line">        f = zipfile.ZipFile(self.outfile, <span class="string">'w'</span>, zipfile.ZIP_DEFLATED)</div><div class="line">        f.write(self.infile)</div><div class="line">        f.close()</div><div class="line">        print(<span class="string">'Finished background zip of:'</span>, self.infile)</div><div class="line"></div><div class="line">background = AsyncZip(<span class="string">'mydata.txt'</span>, <span class="string">'myarchive.zip'</span>)</div><div class="line">background.start()</div><div class="line">print(<span class="string">'The main program continues to run in foreground.'</span>)</div><div class="line"></div><div class="line">background.join()    <span class="comment"># Wait for the background task to finish</span></div><div class="line">print(<span class="string">'Main program waited until background was done.'</span>)</div></pre></td></tr></table></figure>

<p>本文描述的爬虫主要功能是在给定的某个主页下进行广度搜索，找到子页面和父页面中包含的所有pdf和doc/docx文档并下载。之所以做这样的一个工具是因为最近在看一些论文，经常一些学术性网站会放出论文的pdf版本，不过由于pdf文件在网页中分布比较分散，手工下载起来比较麻烦，因此尝试自动爬取网页中的这些资源，然后再逐个检索。<br>爬虫的GUI框架使用<a href="http://www.tkdocs.com/tutorial/index.html" target="_blank" rel="external">Tkinter</a>，Tkinter支持很多语言，比如ruby，perl，python等，是一个比较简单图形界面库，之所以不采用其他第三方GUI框架是因为这些框架很多只支持python2.7.*以前的版本，而我这里用的python3.4.1，无奈选择了最方便的方法。<br>下面是程序的界面：<br><img src="/imgs/爬虫主界面.PNG" alt="主界面" title="主界面"><br><img src="/imgs/爬虫搜索时.PNG" alt="运行时" title="运行时"><br>下载文件的存储路径在设置按钮内设置，界面真心丑，不过能用…</p>
<p><strong>程序的源码在<a href="https://github.com/pangge/python-crawler-ccw" target="_blank" rel="external">我的github</a>上，欢迎大家交流学习。</strong></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="http://abstrusegoose.com/strips/how_stuff_works.png" alt="" title="silly man"><br>最近在学习python语言，python以前没有接触，只用过perl，以前使用perl做一些大的数据集处理，当时也是比较生疏，所以一上来看了简单的官方说明文档，就马上开始coding，大约一周基本就对perl的特性比较熟悉了。所以这次我秉持着从实践中学习技术的角度，打算用python做一些小程序，顺便熟悉python语言的各个方面的特性，也因为我对网络爬虫一直都很有些兴趣，就打算着手做个小工具。<br>]]>
    
    </summary>
    
      <category term="pthon3.4.1" scheme="http://goldencui.org/tags/pthon341/"/>
    
      <category term="网络爬虫" scheme="http://goldencui.org/tags/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/"/>
    
      <category term="python多线程" scheme="http://goldencui.org/tags/python%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
    
      <category term="python" scheme="http://goldencui.org/categories/python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[windows64位环境下python安装numpy、scipy和matplotlib]]></title>
    <link href="http://goldencui.org/2014/10/02/windows64%E4%BD%8D%E7%8E%AF%E5%A2%83%E4%B8%8Bpython%E5%AE%89%E8%A3%85numpy%E3%80%81scipy%E5%92%8Cmatplotlib/"/>
    <id>http://goldencui.org/2014/10/02/windows64位环境下python安装numpy、scipy和matplotlib/</id>
    <published>2014-10-02T08:47:04.000Z</published>
    <updated>2014-10-15T06:52:06.000Z</updated>
    <content type="html"><![CDATA[<hr>
<p>最近想使用python做一些机器学习方向的算法实现，使用python做数据分析和矩阵运算什么的常需要三个库文件：numpy、scipy和matplotlib，于是着手安装。<br>我自己机子的配置是win8+64位操作系统，python安装的版本是3.4.1。安装这几个库首先安装numpy，然后是scipy和matplotlib。<br>查询了numpy的官网后发现根本没有64位，3.4版本python的release版本包，可能是python3.4刚更新的缘故，SourceForge的更新比较慢，还没有最新的发布。scipy和matplotlib也相同，也真是痛苦。<br>上网搜了一些解决方案，当时以为比较简单的问题，直接baidu，结果真是失望，搜索出来的都是无关紧要的内容（原谅我没有google…）。<br>终于在stackoverflow里查到了一些方案，其中一个<a href="http://stackoverflow.com/questions/11200137/installing-numpy-on-64bit-windows-7-with-python-2-7-3" target="_blank" rel="external">install numpy on 64bit win7 with python2.7.3</a>，里面提示了一个<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy" target="_blank" rel="external">资源网站</a>，网站里发布了根据<a href="https://software.intel.com/en-us/intel-mkl" target="_blank" rel="external">Intel® Math Kernel Library</a>第三方生成的最新的python库，进去看了一下，里面很多python可使用的编译好的库。<br>由于我自己python版本是3.4.1，于是选择了这几个文件：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">numpy-MKL-<span class="number">1.9</span>.<span class="number">0</span>.win-amd64-<span class="keyword">py3</span>.<span class="number">4</span>.<span class="keyword">exe</span></div><div class="line">SciPy-<span class="number">0.13</span>.<span class="number">2</span>.win-AMD64-<span class="keyword">py3</span>.<span class="number">4</span>.<span class="keyword">exe</span></div><div class="line">matplotlib-<span class="number">1.4</span>.<span class="number">0</span>.win-amd64-<span class="keyword">py3</span>.<span class="number">4</span>.<span class="keyword">exe</span></div></pre></td></tr></table></figure>

<p><a id="more"></a>安装numpy的时候一路点进去，安装快要结束的时候出现安装程序崩溃的问题。当时吓了一跳，以为是安装程序不匹配，没办法自己搞虚拟机准备用linux环境下的解决方案。后面无意间在windows下测试了一下numpy是否安装成功<br>结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;<span class="keyword">from</span> numpy <span class="keyword">import</span> <span class="keyword">import</span> *</div><div class="line">&gt;&gt;&gt;a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</div><div class="line">&gt;&gt;&gt;mat(a)</div><div class="line">&gt;&gt;&gt;matrix([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]])</div></pre></td></tr></table></figure>

<p>艾玛!!!  竟然没有问题，证明numpy安装应该没有错,可以使用，scipy和mat的安装也出现类似问题，不过暂时都不影响使用，不知道后续会不会出问题。之所以程序安装会出现崩溃，我怀疑应该我自己win8系统的错误或者库的兼容性问题。后面还没有完整测试这几个库，不知道是不是其它电脑也会出现类似问题，唉也毕竟是第三方的库，多有不兼容的地方还是很正常的。</p>
<p>linux环境下的话，可以下载numpy和scipy的源码直接自己编译，不过需要安装MKL，因为本人已经不用linux很久了，所以也没有仔细阅读具体方法，<a href="https://software.intel.com/zh-cn/articles/numpyscipy-with-intel-mkl" target="_blank" rel="external">这里</a>有很清晰的流程说明。</p>
]]></content>
    <summary type="html">
    <![CDATA[<hr>
<p>最近想使用python做一些机器学习方向的算法实现，使用python做数据分析和矩阵运算什么的常需要三个库文件：numpy、scipy和matplotlib，于是着手安装。<br>我自己机子的配置是win8+64位操作系统，python安装的版本是3.4.1。安装这几个库首先安装numpy，然后是scipy和matplotlib。<br>查询了numpy的官网后发现根本没有64位，3.4版本python的release版本包，可能是python3.4刚更新的缘故，SourceForge的更新比较慢，还没有最新的发布。scipy和matplotlib也相同，也真是痛苦。<br>上网搜了一些解决方案，当时以为比较简单的问题，直接baidu，结果真是失望，搜索出来的都是无关紧要的内容（原谅我没有google…）。<br>终于在stackoverflow里查到了一些方案，其中一个<a href="http://stackoverflow.com/questions/11200137/installing-numpy-on-64bit-windows-7-with-python-2-7-3" target="_blank" rel="external">install numpy on 64bit win7 with python2.7.3</a>，里面提示了一个<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy" target="_blank" rel="external">资源网站</a>，网站里发布了根据<a href="https://software.intel.com/en-us/intel-mkl" target="_blank" rel="external">Intel® Math Kernel Library</a>第三方生成的最新的python库，进去看了一下，里面很多python可使用的编译好的库。<br>由于我自己python版本是3.4.1，于是选择了这几个文件：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">numpy-MKL-<span class="number">1.9</span>.<span class="number">0</span>.win-amd64-<span class="keyword">py3</span>.<span class="number">4</span>.<span class="keyword">exe</span></div><div class="line">SciPy-<span class="number">0.13</span>.<span class="number">2</span>.win-AMD64-<span class="keyword">py3</span>.<span class="number">4</span>.<span class="keyword">exe</span></div><div class="line">matplotlib-<span class="number">1.4</span>.<span class="number">0</span>.win-amd64-<span class="keyword">py3</span>.<span class="number">4</span>.<span class="keyword">exe</span></div></pre></td></tr></table></figure>

<p>]]>
    
    </summary>
    
      <category term="numpy" scheme="http://goldencui.org/tags/numpy/"/>
    
      <category term="scipy" scheme="http://goldencui.org/tags/scipy/"/>
    
      <category term="matplotlib" scheme="http://goldencui.org/tags/matplotlib/"/>
    
      <category term="python" scheme="http://goldencui.org/tags/python/"/>
    
      <category term="win64" scheme="http://goldencui.org/tags/win64/"/>
    
      <category term="python" scheme="http://goldencui.org/categories/python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[基于FPGA的SOPC系统设计tips]]></title>
    <link href="http://goldencui.org/2014/09/26/%E5%9F%BA%E4%BA%8EFPGA%E7%9A%84SOPC%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1tips/"/>
    <id>http://goldencui.org/2014/09/26/基于FPGA的SOPC系统设计tips/</id>
    <published>2014-09-26T09:57:53.000Z</published>
    <updated>2014-10-15T06:54:55.000Z</updated>
    <content type="html"><![CDATA[<p> 之前做了半年的基于FPGA的SOPC的研究，期间遇到很多问题，对于我这个以前只搞过软件的人来说简直是一种折磨，从最简单的串口信息获取，到FPGA外设驱动,这期间遇到很多困难，好在老天有眼一一克服，这篇文章零散的记录基于FPGA嵌入式PPC440微处理器+standalone模式下系统编程方面的操作，以tips为形式展现，只列出系统构建过程中遇到的问题以及解决方法。一些XILINX官网可以查到的技术资料这里暂且不提，后面会附上链接。</p>
<h2 id="1-_浮点数据串口输出">1. 浮点数据串口输出</h2>
<p>XILINX公司提供的标准c一部分实现，并不是所有。它提供的标准串口输出函数包括xil_printf(),printnum();前一个函数和标准c相似，printnum()输出数字型数据。但是自己项目中涉及到浮点型数据的展示，所以想着xil_printf(“%f”,float_data)，发现根本不行，检查了xil_print()的源码，发现根本没有浮点输出的模式（orz…）。<br>仔细检查了XILINX SDK的技术文档，发现它提供了另一个标准函数的实现sprintf()，和print()函数，print()函数就是打印字符类型数据，所以我们可以把浮点类型数据转变为char*字符串，然后输出。所以包装一下，加入了double型和float型数据的打印程序：<br><a id="more"></a></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">void</span> print_double(<span class="keyword">double</span> f)</div><div class="line">{</div><div class="line">	<span class="keyword">char</span> out[<span class="number">40</span>];</div><div class="line">	<span class="keyword">char</span> *output=out;</div><div class="line">	<span class="built_in">sprintf</span>(output,<span class="string">"%.3f"</span>,f);</div><div class="line">	print(out);</div><div class="line">}</div><div class="line"></div><div class="line"><span class="keyword">void</span> print_float(<span class="keyword">float</span> f)</div><div class="line">{</div><div class="line">	<span class="keyword">char</span> out[<span class="number">20</span>];</div><div class="line">	<span class="keyword">char</span> *output=out;</div><div class="line">	<span class="built_in">sprintf</span>(output,<span class="string">"%.3f"</span>,f);</div><div class="line">	print(out);</div><div class="line">}</div></pre></td></tr></table></figure>

<h2 id="2-_Sqrt函数问题">2. Sqrt函数问题</h2>
<p>程序中遇到过一个地方需要sqrt()调用，一般思路是：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Double_result=<span class="built_in">sqrt</span>(double_data);</div></pre></td></tr></table></figure>

<p>程序一运行我就哭了，崩溃。。。。仔细测试了很多方法，发现这种模式可行：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">u32_result=<span class="built_in">sqrt</span>(u32_data);</div></pre></td></tr></table></figure>

<p>可惜u32类型类似整形，会对小数位截断，这样得不到精确的解，于是尝试改进得到一个可以运行的模式：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">u32_result=<span class="built_in">sqrt</span>((<span class="keyword">double</span>)u32_data);</div></pre></td></tr></table></figure>

<p>可行，但是还不满意，得到结果还是整形；突然想到我们系统需求是小数点2位精确，这样的话可以采取这样的形式解决：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">u32_data=u32_data*<span class="number">10000</span>;</div><div class="line">u32_result=<span class="built_in">sqrt</span>(u32_data);</div><div class="line">double_result=(<span class="keyword">double</span>)u32_result/<span class="number">100</span>;</div></pre></td></tr></table></figure>

<h2 id="3-_DDR2_读写_">3. DDR2 读写 </h2>
<p>首先在硬件系统结构设计的时候，加入DDR2外设，得到DDR2存储的base_address，然后再standalone模式中加入实现。<br>读：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">u32_DDR2_addr = base_address; (例如：<span class="number">0x00000000</span>)</div><div class="line">u32_word = u32_DDR2_addr[u32_index];</div></pre></td></tr></table></figure>

<p>写：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">u32_DDR2_addr[u32_index] = u32_word;</div></pre></td></tr></table></figure>

<h2 id="4-_PPC440_微处理器时间获取">4. PPC440 微处理器时间获取</h2>
<p>在XPS上进行硬件系统设计的采用的PPC440主频是125MHZ。XILINX内并未实现标准操作系统的timer功能，所以查阅了XILINX提供的源码找到一种提供计时的方法：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">u32 ReadTimer()</div><div class="line">{</div><div class="line">	u32 timeCount;</div><div class="line">	timeCount=XTmrCtr_ReadReg(InstancePtr.BaseAddress,</div><div class="line">					 TmrCtrNumber, XTC_TCR_OFFSET);</div><div class="line">	<span class="keyword">return</span> timeCount;</div><div class="line">}</div><div class="line"></div><div class="line"><span class="keyword">void</span> InitTimer()</div><div class="line">{</div><div class="line">	InstancePtr.BaseAddress=TIMER_ADDR; <span class="comment">//timer addr</span></div><div class="line">	TmrCtrNumber=TIMER_NUM; <span class="comment">// timer number</span></div><div class="line">	<span class="comment">/*</span></div><div class="line">	 * Reset the timer and the interrupt</div><div class="line">	 */</div><div class="line">	XTmrCtr_WriteReg(InstancePtr.BaseAddress, TmrCtrNumber,</div><div class="line">			  XTC_TCSR_OFFSET,</div><div class="line">			  XTC_CSR_INT_OCCURED_MASK | XTC_CSR_LOAD_MASK);</div><div class="line"></div><div class="line"></div><div class="line">	<span class="comment">/*</span></div><div class="line">	 * Set the control/status register to enable timer</div><div class="line">	 */</div><div class="line">	XTmrCtr_WriteReg(InstancePtr.BaseAddress, TmrCtrNumber,</div><div class="line">			  XTC_TCSR_OFFSET, XTC_CSR_ENABLE_TMR_MASK);</div><div class="line">}</div></pre></td></tr></table></figure>

<p>使用计时方法的时候首先需要调用一次InitTimer()，然后在计数器开启的状态下可以调用ReadTimer()读计数器寄存器.<br>例：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">InitTimer(); <span class="comment">// initial  timer	</span></div><div class="line">Todo module</div><div class="line">u32_TimerCount =ReadTimer();</div></pre></td></tr></table></figure>

<p>这里得到的u32_TimerCount是PPC的指令计数，得到精确的描述还需要如下处理：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">u32 sec_time = u32_TimerCount/<span class="number">125000000</span>; <span class="comment">//(ppc440 主频125MHZ)</span></div></pre></td></tr></table></figure>

<p>资源：<br><a href="http://tgoogle.xilinx.com/search?q=SDK+DVI&amp;btnG=New+Search&amp;getfields=*&amp;numgm=5&amp;filter=0&amp;proxystylesheet=support&amp;client=support&amp;getfields=*&amp;num=200&amp;oe=UTF-8&amp;ie=UTF-8&amp;output=xml_no_dtd&amp;requiredfields=-Archived%3Atrue&amp;show_dynamic_navigation=1&amp;sort=date%3AD%3AL%3Ad1&amp;lang2search=&amp;wc=200&amp;wc_mc=1&amp;ud=1&amp;exclude_apps=1&amp;site=Answers_Docs_Forums" target="_blank" rel="external">xilinx 资源搜索</a><br><a href="http://www.xilinx.com/support/documentation/sw_manuals/xilinx12_1/SDK_Doc/tasks/sdk_t_create_new_appln.htm" target="_blank" rel="external">xilnx sdk嵌入式开发 官方指导</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p> 之前做了半年的基于FPGA的SOPC的研究，期间遇到很多问题，对于我这个以前只搞过软件的人来说简直是一种折磨，从最简单的串口信息获取，到FPGA外设驱动,这期间遇到很多困难，好在老天有眼一一克服，这篇文章零散的记录基于FPGA嵌入式PPC440微处理器+standalone模式下系统编程方面的操作，以tips为形式展现，只列出系统构建过程中遇到的问题以及解决方法。一些XILINX官网可以查到的技术资料这里暂且不提，后面会附上链接。</p>
<h2 id="1-_浮点数据串口输出">1. 浮点数据串口输出</h2>
<p>XILINX公司提供的标准c一部分实现，并不是所有。它提供的标准串口输出函数包括xil_printf(),printnum();前一个函数和标准c相似，printnum()输出数字型数据。但是自己项目中涉及到浮点型数据的展示，所以想着xil_printf(“%f”,float_data)，发现根本不行，检查了xil_print()的源码，发现根本没有浮点输出的模式（orz…）。<br>仔细检查了XILINX SDK的技术文档，发现它提供了另一个标准函数的实现sprintf()，和print()函数，print()函数就是打印字符类型数据，所以我们可以把浮点类型数据转变为char*字符串，然后输出。所以包装一下，加入了double型和float型数据的打印程序：<br>]]>
    
    </summary>
    
      <category term="SOPC" scheme="http://goldencui.org/tags/SOPC/"/>
    
      <category term="XILINX" scheme="http://goldencui.org/tags/XILINX/"/>
    
      <category term="PPC" scheme="http://goldencui.org/tags/PPC/"/>
    
      <category term="嵌入式" scheme="http://goldencui.org/categories/%E5%B5%8C%E5%85%A5%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[git基础命令总结]]></title>
    <link href="http://goldencui.org/2014/07/28/git%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/"/>
    <id>http://goldencui.org/2014/07/28/git基础命令总结/</id>
    <published>2014-07-27T16:15:26.000Z</published>
    <updated>2014-10-15T06:52:38.000Z</updated>
    <content type="html"><![CDATA[<h4 id="（针对ssh方式）"><em>（针对ssh方式）</em></h4>
<hr>
<h1 id="1_github_代码提交">1  github 代码提交</h1>
<p><strong>创建新的版本库</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">touch README.md</div><div class="line"><span class="variable">$git</span> init</div><div class="line"><span class="variable">$git</span> add README.md</div><div class="line"><span class="variable">$git</span> commit -<span class="keyword">m</span> <span class="string">"first commit"</span></div><div class="line"><span class="variable">$git</span> remote add origin git<span class="variable">@github</span>.com:youname/youproject.git</div><div class="line"><span class="variable">$git</span> <span class="keyword">push</span> -u origin master</div></pre></td></tr></table></figure>

<p><strong>推送现有的版本库</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$git</span> remote add origin git<span class="variable">@github</span>.com:youname/youproject.git</div><div class="line"><span class="variable">$git</span> <span class="keyword">push</span> -u origin master</div></pre></td></tr></table></figure>

<p><strong>添加修改的版本库</strong><br><a id="more"></a></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$git</span> add . <span class="comment">// .表示添加所有文件 </span></div><div class="line"><span class="variable">$git</span> commit -m <span class="string">"修改日志"</span></div><div class="line"><span class="variable">$git</span> push -u origin master</div></pre></td></tr></table></figure>

<p><strong><em>错误解决</em></strong></p>
<hr>
<h4 id="1-运行：">1.运行：</h4>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="title">git</span> remote add origin git<span class="variable">@github</span>.com:youname/youproject.git</div></pre></td></tr></table></figure>

<p>错误提示：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">fata<span class="variable">l:</span> remote origin already <span class="built_in">exists</span>.</div></pre></td></tr></table></figure>

<p>解决办法：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$git</span> remote rm origin</div></pre></td></tr></table></figure>

<h4 id="2-运行：">2.运行：</h4>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="variable">$ </span>git push origin master</div></pre></td></tr></table></figure>

<p>错误提示：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">error</span>:failed <span class="keyword">to</span> push som refs <span class="keyword">to</span></div></pre></td></tr></table></figure>

<p>解决办法：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ git pull origin master <span class="comment">// 先把远程服务器中版本拉取下来，再push</span></div></pre></td></tr></table></figure>

<hr>
<h1 id="2_github_代码clone">2  github 代码clone</h1>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$ </span>git clone git<span class="variable">@github</span>.<span class="symbol">com:</span>youname/youproject.git  <span class="string">"path"</span> /<span class="regexp">/ path 要存放本地版本库的地址</span></div></pre></td></tr></table></figure>

<p><img src="http://github.global.ssl.fastly.net/images/modules/logos_page/Octocat.png" alt="GitHub" title="GitHub cat"></p>
]]></content>
    <summary type="html">
    <![CDATA[<h4 id="（针对ssh方式）"><em>（针对ssh方式）</em></h4>
<hr>
<h1 id="1_github_代码提交">1  github 代码提交</h1>
<p><strong>创建新的版本库</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">touch README.md</div><div class="line"><span class="variable">$git</span> init</div><div class="line"><span class="variable">$git</span> add README.md</div><div class="line"><span class="variable">$git</span> commit -<span class="keyword">m</span> <span class="string">"first commit"</span></div><div class="line"><span class="variable">$git</span> remote add origin git<span class="variable">@github</span>.com:youname/youproject.git</div><div class="line"><span class="variable">$git</span> <span class="keyword">push</span> -u origin master</div></pre></td></tr></table></figure>

<p><strong>推送现有的版本库</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$git</span> remote add origin git<span class="variable">@github</span>.com:youname/youproject.git</div><div class="line"><span class="variable">$git</span> <span class="keyword">push</span> -u origin master</div></pre></td></tr></table></figure>

<p><strong>添加修改的版本库</strong><br>]]>
    
    </summary>
    
      <category term="github" scheme="http://goldencui.org/tags/github/"/>
    
      <category term="代码托管" scheme="http://goldencui.org/categories/%E4%BB%A3%E7%A0%81%E6%89%98%E7%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[(ZZ)休息，休息一会儿]]></title>
    <link href="http://goldencui.org/2014/07/26/%E4%BC%91%E6%81%AF%EF%BC%8C%E4%BC%91%E6%81%AF%E4%B8%80%E4%BC%9A%E5%84%BF/"/>
    <id>http://goldencui.org/2014/07/26/休息，休息一会儿/</id>
    <published>2014-07-26T04:01:39.000Z</published>
    <updated>2014-09-26T11:07:35.000Z</updated>
    <content type="html"><![CDATA[<p><img src="http://www.yinwang.org/images/yixiu.jpg" alt="" title="一休哥 1"></p>
<p>本人进入了比较长的，理所应得的休息和娱乐时间。无聊时也看看闲书和电影。这里推荐几个最近看的东西。</p>
<h1 id="《The_Design_of_Everyday_Things》">《The Design of Everyday Things》</h1>
<hr>
<p>最近给我最大影响的是这本1988年出版的《<a href="http://www.amazon.com/Design-Everyday-Things-Revised-Expanded-ebook/dp/B00E257T6C" target="_blank" rel="external">The Design of Everyday Things</a>》（简称DOET）。有趣的是，它的作者 Don Norman 曾经是 Apple Fellow，也是《<a href="http://web.mit.edu/~simsong/www/ugh.pdf" target="_blank" rel="external">The Unix-Haters Handbook</a>》一书序言的作者。<br><a id="more"></a><br>DOET 不但包含并且支持了我的博文《<a href="http://www.yinwang.org/blog-cn/2014/04/11/hacker-culture" target="_blank" rel="external">黑客文化的精髓</a>》以及《<a href="http://www.yinwang.org/blog-cn/2014/01/25/pl-and" target="_blank" rel="external">程序语言与……</a>》里的基本观点，而且提出了比《<a href="http://www.yinwang.org/blog-cn/2012/05/18/user-friendliness" target="_blank" rel="external">什么是“对用户友好”</a>》更精辟可行的解决方案。</p>
<p>我觉得这应该是每个程序员必读的书籍。为什么每个程序员必读呢？因为虽然这本书是设计类专业的必读书籍，而计算机及其编程语言和工具，其实才是作者指出的缺乏设计思想的“重灾区”。看了它，你会发现很多所谓的“人为错误”，其实是工具的设计不合理造成的。一个设计良好的工具，应该只需要很少量的文档甚至不需要文档。这本书将提供给你改进一切事物的原则和灵感。你会恢复你的人性。</p>
<p>值得一提的是，虽然 Don Norman 曾经是 Apple Fellow，但我觉得 Apple 产品设计的人性化程度与 Norman 大叔的思维高度还是有一定的差距的。</p>
<p>如果你跟我一样不想用眼睛看书，可以到 Audible 买本<a href="http://www.audible.com/pd/Science-Technology/The-Design-of-Everyday-Things-Audiobook/B005I5MDGQ" target="_blank" rel="external">有声书</a>来听。</p>
<h1 id="《The_Conquest_of_Happiness》">《The Conquest of Happiness》</h1>
<hr>
<p>每个人都想得到快乐，但是他们往往误解了快乐的来源，追求了错误的东西，所以大多数人因此得到的是痛苦，并且给其他人带来痛苦。英国哲学家和数学家罗素写于1930年的《<a href="http://www.amazon.com/The-Conquest-Happiness-Bertrand-Russell/dp/0871401622" target="_blank" rel="external">The Conquest of Happiness</a>》就是彻底的分析这些2014现代人的常见问题的。</p>
<p>在第一部分，罗素透彻的分析了几个常见的不快乐的原因：看破红尘，竞争，过度追求刺激，疲劳，嫉妒，罪恶感，被害妄想症，…… 第二部分，他提出了得到快乐的有效方法。</p>
<p>如果你认为自己没有这些问题，或者认为自己懂得这些是怎么回事，请再次反思一下，因为每个人都或多或少有这些问题。特别是我发现，竞争和攀比所带来的不快乐，在中国人里面是很普遍的现象。</p>
<p>另外，很多心理学家，特别是所谓“正向心理学”（positive psychology），也声称研究如何使人快乐，但我发现他们很多只是扯着“快乐”的幌子，开发自己的市场。罗素的思想比他们深刻很多。</p>
<h1 id="大独裁者">大独裁者</h1>
<hr>
<p>谈到人性，我推荐卓别林在电影《<a href="https://www.youtube.com/watch?v=6FMNFvKEy4c" target="_blank" rel="external">大独裁者</a>》里面的最后演讲。他引起了我对技术的价值的思考。有人说，世界不是毁在疯子手里就是毁在工作狂手里，是有一定的道理的。</p>
<h1 id="摩登时代">摩登时代</h1>
<hr>
<p>其实比《大独裁者》更幽默，更有趣，对现代社会更有意义的，是卓别林的《<a href="http://www.amazon.com/Modern-Times-Charlie-Chaplin/dp/B004DARF6A" target="_blank" rel="external">摩登时代</a>》。这样一部1930年代的黑白无声电影，道出了直到2014年的今天，世界上最大的问题：过度工作。<br>现代社会很多人为了所谓的“生存”，把自己变成了一台盲目不停工作的机器。加班加点的干活，并且还试图让别人也变成跟他一样。一切都是为了工作，为了效率，为了“优秀”，为了出人头地。太多的野心，太多的目标，却对身边最简单的乐趣视而不见。试试放慢匆忙的脚步，思考一下自己在干什么吧！</p>
<p>因为这些原因，我继续睡觉，这是拯救世界的最好办法 zZZZ</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="http://www.yinwang.org/images/yixiu.jpg" alt="" title="一休哥 1"></p>
<p>本人进入了比较长的，理所应得的休息和娱乐时间。无聊时也看看闲书和电影。这里推荐几个最近看的东西。</p>
<h1 id="《The_Design_of_Everyday_Things》">《The Design of Everyday Things》</h1>
<hr>
<p>最近给我最大影响的是这本1988年出版的《<a href="http://www.amazon.com/Design-Everyday-Things-Revised-Expanded-ebook/dp/B00E257T6C" target="_blank" rel="external">The Design of Everyday Things</a>》（简称DOET）。有趣的是，它的作者 Don Norman 曾经是 Apple Fellow，也是《<a href="http://web.mit.edu/~simsong/www/ugh.pdf" target="_blank" rel="external">The Unix-Haters Handbook</a>》一书序言的作者。<br>]]>
    
    </summary>
    
      <category term="hacker" scheme="http://goldencui.org/tags/hacker/"/>
    
      <category term="转载" scheme="http://goldencui.org/categories/%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
</feed>
